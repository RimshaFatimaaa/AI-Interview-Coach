{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fae434-d74b-4691-9da9-ae4de3d38b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading generation pipeline model 'google/flan-t5-base' (this may download files)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 — LLMs demo (generate + evaluate).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "INFO:__main__:Generating question with local model...\n",
      "INFO:__main__:Loading embedding model 'all-MiniLM-L6-v2' (this may download files)...\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated question: What is the job description for the candidate? (source: local_flan_t5 )\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3d6280becb4ac0a26f142731a6b6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rimsha\\anaconda3\\envs\\llms_course_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rimsha\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa11cdfdc6814ebf889ec00620edfbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee78c139cb294e0593b41f0599594f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad963d6ca764add93c144925f038086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4760a52bc3cf4adf926a2ccd1d330fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c799e28b63c64ab19815e28d4a721326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e56b56638947dfb448c8121e35e9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816f8d87c7e24b78b963a707e177699f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462f12d8f8d94202aff565fdd5a626b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db495cb92de4cdea4ef6cd270ce70f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11e274ace654ea6b283ea2ef6860b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Computing embeddings for canonical answers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5849107e104c7cba0b05696bc3a9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17104e81fc2e40aabd3123478631127e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Running local LLM evaluator (may be less strict than OpenAI)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question\": \"What is the job description for the candidate?\",\n",
      "  \"candidate_answer\": \"Decorators are functions that wrap another function to extend its behavior without modifying it. They are useful for logging and caching.\",\n",
      "  \"matched_canonical\": {\n",
      "    \"id\": \"tech_02\",\n",
      "    \"answer\": \"Decorators are higher-order functions that modify other functions or methods. Use them for logging, access control, caching, or adding behavior without changing the original function.\"\n",
      "  },\n",
      "  \"similarity\": 0.9714755763973403,\n",
      "  \"evaluation\": {\n",
      "    \"relevance\": 0.9714755763973403,\n",
      "    \"completeness\": 0.9714755763973403,\n",
      "    \"clarity\": 1.0,\n",
      "    \"feedback\": \"Similarity to canonical answer: 0.97. Expand more on examples and outcomes.\",\n",
      "    \"suggestions\": \"Use STAR structure for behavioral answers; add specific examples and metrics.\",\n",
      "    \"matched_canonical_id\": \"tech_02\",\n",
      "    \"similarity\": 0.9714755763973403\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "step2_llms.py\n",
    "Single-file implementation for Step 2 (LLMs / Transformers) of:\n",
    "AI-Powered Job Interview Coach — PRD\n",
    "\n",
    "Features:\n",
    "- Dynamic question generation (HR & Technical) via OpenAI or local FLAN-T5.\n",
    "- Answer evaluation:\n",
    "    - Embedding-based similarity to canonical answers (sentence-transformers).\n",
    "    - LLM-driven rubric scoring & textual feedback (OpenAI or FLAN-T5).\n",
    "- Minimal FastAPI endpoints for integration.\n",
    "\n",
    "Usage:\n",
    "  - Optional: set OPENAI_API_KEY to use OpenAI Chat completions for generation/evaluation.\n",
    "  - Otherwise the code falls back to local HF models (may download model files on first run).\n",
    "  - Run server:\n",
    "      uvicorn step2_llms:app --reload\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict, Optional\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from pydantic import BaseModel\n",
    "from fastapi import FastAPI\n",
    "import logging\n",
    "\n",
    "# OPTIONAL: OpenAI\n",
    "try:\n",
    "    import openai\n",
    "except Exception:\n",
    "    openai = None\n",
    "\n",
    "# sentence-transformers for embeddings\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception:\n",
    "    SentenceTransformer = None\n",
    "\n",
    "# transformers pipeline for local generation (FLAN-T5)\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "except Exception:\n",
    "    pipeline = None\n",
    "\n",
    "# ---------- Configuration ----------\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\", None)\n",
    "if OPENAI_KEY and openai:\n",
    "    openai.api_key = OPENAI_KEY\n",
    "\n",
    "# Model choices (change if you prefer other models)\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"   # sentence-transformers (light + fast). :contentReference[oaicite:2]{index=2}\n",
    "LOCAL_TEXT2TEXT_MODEL = \"google/flan-t5-base\"   # used when OpenAI not configured. :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "# Caching model objects\n",
    "_embedding_model = None\n",
    "_generation_pipeline = None\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "\n",
    "def safe_load_embedding_model():\n",
    "    global _embedding_model\n",
    "    if _embedding_model is None:\n",
    "        if SentenceTransformer is None:\n",
    "            raise RuntimeError(\"sentence-transformers is required. Install with `pip install sentence-transformers`.\")\n",
    "        logger.info(f\"Loading embedding model '{EMBEDDING_MODEL_NAME}' (this may download files)...\")\n",
    "        _embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    return _embedding_model\n",
    "\n",
    "def safe_load_generation_pipeline():\n",
    "    global _generation_pipeline\n",
    "    if _generation_pipeline is None:\n",
    "        if pipeline is None:\n",
    "            raise RuntimeError(\"transformers is required. Install with `pip install transformers[torch]`.\")\n",
    "        logger.info(f\"Loading generation pipeline model '{LOCAL_TEXT2TEXT_MODEL}' (this may download files)...\")\n",
    "        # Use text2text-generation pipeline for FLAN-T5\n",
    "        _generation_pipeline = pipeline(task=\"text2text-generation\", model=LOCAL_TEXT2TEXT_MODEL, device=0 if _has_cuda() else -1)\n",
    "    return _generation_pipeline\n",
    "\n",
    "def _has_cuda():\n",
    "    try:\n",
    "        import torch\n",
    "        return torch.cuda.is_available()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s or \"\"\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    # safe cosine similarity\n",
    "    a = np.array(a, dtype=float)\n",
    "    b = np.array(b, dtype=float)\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "# ---------- Sample canonical Q&A store ----------\n",
    "# In production this would be in your Step 5 vector DB; here we keep a small in-memory sample.\n",
    "CANONICAL_QA = [\n",
    "    {\"id\": \"hr_01\", \"round\": \"hr\", \"question\": \"Tell me about a time you worked in a team and faced conflict. How did you handle it?\",\n",
    "     \"answer\": \"I used the STAR method: Situation - our team missed a deadline; Task - I coordinated communication; Action - scheduled a meeting, redistributed tasks, negotiated new timelines; Result - we delivered with acceptable quality and learned to align better.\"},\n",
    "    {\"id\": \"tech_01\", \"round\": \"technical\", \"question\": \"Explain the concept of object-oriented programming and its main principles.\",\n",
    "     \"answer\": \"OOP organizes code into objects that combine data and behavior. Main principles: Encapsulation (bundling data + methods), Abstraction (exposing only necessary details), Inheritance (reuse and extend classes), Polymorphism (same interface, different implementations).\"},\n",
    "    {\"id\": \"tech_02\", \"round\": \"technical\", \"question\": \"What are Python decorators and when would you use them?\",\n",
    "     \"answer\": \"Decorators are higher-order functions that modify other functions or methods. Use them for logging, access control, caching, or adding behavior without changing the original function.\"},\n",
    "    # Add more canonical items as needed...\n",
    "]\n",
    "\n",
    "# Precompute embeddings for canonical answers (lazy)\n",
    "_canonical_embeddings = None\n",
    "_canonical_loaded = False\n",
    "\n",
    "def ensure_canonical_embeddings():\n",
    "    global _canonical_embeddings, _canonical_loaded\n",
    "    if _canonical_loaded:\n",
    "        return\n",
    "    model = safe_load_embedding_model()\n",
    "    texts = [normalize_text(item[\"answer\"]) for item in CANONICAL_QA]\n",
    "    logger.info(\"Computing embeddings for canonical answers...\")\n",
    "    _canonical_embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "    _canonical_loaded = True\n",
    "\n",
    "# ---------- Question Generation ----------\n",
    "\n",
    "def _generate_question_openai(round_type: str, context: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate a question using OpenAI Chat API (if available).\n",
    "    \"\"\"\n",
    "    assert openai is not None, \"OpenAI library not available.\"\n",
    "    prompt = f\"You are an interview question generator. Produce one concise {round_type} interview question.\"\n",
    "    if context:\n",
    "        prompt += f\" Context: {context}\"\n",
    "    prompt += \" Keep the question clear and relevant to the candidate's skills. Return only the question.\"\n",
    "\n",
    "    logger.info(\"Calling OpenAI to generate question...\")\n",
    "    resp = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\" if \"gpt-4o-mini\" in (openai.Model.list().data[0].id if hasattr(openai, 'Model') else \"\") else \"gpt-4o\",\n",
    "        messages=[{\"role\":\"system\",\"content\":\"You generate single interview questions.\"},\n",
    "                  {\"role\":\"user\",\"content\":prompt}],\n",
    "        temperature=0.7,\n",
    "        max_tokens=120,\n",
    "    )\n",
    "    # Try to extract text\n",
    "    text = resp[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    return text\n",
    "\n",
    "def _generate_question_local(round_type: str, context: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate a question using local FLAN-T5 text2text pipeline.\n",
    "    \"\"\"\n",
    "    pipe = safe_load_generation_pipeline()\n",
    "    prompt = f\"Generate one short {round_type} interview question.\"\n",
    "    if context:\n",
    "        prompt += f\" Context: {context}\"\n",
    "    prompt += \" Keep it concise. Output only the question.\"\n",
    "    logger.info(\"Generating question with local model...\")\n",
    "    out = pipe(prompt, max_length=128, do_sample=False)\n",
    "    text = out[0][\"generated_text\"].strip()\n",
    "    # FLAN-T5 may echo the prompt; attempt to clean to single sentence\n",
    "    return text\n",
    "\n",
    "def generate_question(round_type: str = \"hr\", context: Optional[str] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Public API for generating a single interview question.\n",
    "    round_type: \"hr\" or \"technical\"\n",
    "    \"\"\"\n",
    "    round_type = round_type.lower()\n",
    "    if OPENAI_KEY and openai:\n",
    "        try:\n",
    "            q = _generate_question_openai(round_type, context)\n",
    "            return {\"question\": normalize_text(q), \"source\": \"openai\"}\n",
    "        except Exception as e:\n",
    "            logger.warning(\"OpenAI generation failed, falling back to local. Error: %s\", e)\n",
    "    # fallback to local\n",
    "    q = _generate_question_local(round_type, context)\n",
    "    return {\"question\": normalize_text(q), \"source\": \"local_flan_t5\"}\n",
    "\n",
    "# ---------- Answer Evaluation ----------\n",
    "\n",
    "def _llm_evaluate_openai(question: str, candidate_answer: str, top_canonical: Dict, similarity_score: float) -> Dict:\n",
    "    \"\"\"\n",
    "    Use OpenAI to produce a structured evaluation. We ask for a JSON output with numeric scores.\n",
    "    \"\"\"\n",
    "    system = \"You are an objective interview evaluator. Given the candidate's answer and a canonical example answer, evaluate using a rubric and output strict JSON with these keys: relevance (0-1), completeness (0-1), clarity (0-1), feedback (string), suggestions (string). Also return matched_canonical_id and similarity (0-1).\"\n",
    "    user_prompt = (\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Candidate answer: {candidate_answer}\\n\\n\"\n",
    "        f\"Canonical answer (best match): {top_canonical['answer']}\\n\\n\"\n",
    "        f\"Similarity (embedding-based): {similarity_score:.4f}\\n\\n\"\n",
    "        \"Evaluate using the rubric. Provide JSON only.\"\n",
    "    )\n",
    "    try:\n",
    "        resp = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o\" if \"gpt-4o\" in (openai.Model.list().data[0].id if hasattr(openai,'Model') else \"\") else \"gpt-4o-mini\",\n",
    "            messages=[{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user_prompt}],\n",
    "            temperature=0.0,\n",
    "            max_tokens=400,\n",
    "        )\n",
    "        content = resp[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        # Try to extract JSON from the model output\n",
    "        json_text = _extract_json_from_text(content)\n",
    "        return json.loads(json_text)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"OpenAI evaluation failed: %s\", e)\n",
    "        # fallback to simple heuristic evaluator below\n",
    "        return _heuristic_evaluation(candidate_answer, top_canonical, similarity_score)\n",
    "\n",
    "def _llm_evaluate_local(question: str, candidate_answer: str, top_canonical: Dict, similarity_score: float) -> Dict:\n",
    "    \"\"\"\n",
    "    Use local FLAN-T5 to produce a JSON-like evaluation. FLAN may not output strict JSON; attempt to parse.\n",
    "    \"\"\"\n",
    "    pipe = safe_load_generation_pipeline()\n",
    "    prompt = (\n",
    "        \"You are an objective interview evaluator. Given the question, candidate answer, and canonical answer, \"\n",
    "        \"produce a JSON object with keys: relevance (0-1), completeness (0-1), clarity (0-1), feedback, suggestions, matched_canonical_id, similarity.\\n\\n\"\n",
    "        f\"Question: {question}\\n\\nCandidate answer: {candidate_answer}\\n\\nCanonical answer: {top_canonical['answer']}\\n\\n\"\n",
    "        f\"Similarity (embedding): {similarity_score:.4f}\\n\\n\"\n",
    "        \"Return only valid JSON.\"\n",
    "    )\n",
    "    logger.info(\"Running local LLM evaluator (may be less strict than OpenAI)...\")\n",
    "    out = pipe(prompt, max_length=512, do_sample=False)\n",
    "    text = out[0][\"generated_text\"].strip()\n",
    "    json_text = _extract_json_from_text(text)\n",
    "    if not json_text:\n",
    "        return _heuristic_evaluation(candidate_answer, top_canonical, similarity_score)\n",
    "    try:\n",
    "        return json.loads(json_text)\n",
    "    except Exception:\n",
    "        return _heuristic_evaluation(candidate_answer, top_canonical, similarity_score)\n",
    "\n",
    "def _heuristic_evaluation(candidate_answer: str, top_canonical: Dict, similarity_score: float) -> Dict:\n",
    "    \"\"\"\n",
    "    If LLM-based evaluation isn't available, return heuristic scores:\n",
    "      - relevance: based on cosine similarity\n",
    "      - completeness: similarity adjusted for length\n",
    "      - clarity: based on average word length / filler heuristics\n",
    "    \"\"\"\n",
    "    cand = normalize_text(candidate_answer)\n",
    "    length = max(1, len(cand.split()))\n",
    "    # heuristics\n",
    "    relevance = float(np.clip(similarity_score, 0.0, 1.0))\n",
    "    completeness = float(np.clip(similarity_score * min(1.0, math.log(1 + length) / 3.0), 0.0, 1.0))\n",
    "    # Naive clarity measure: shorter filler words / punctuation\n",
    "    filler_count = len(re.findall(r\"\\bu(m+|uh+|like|you know)\\b\", cand.lower()))\n",
    "    clarity = float(np.clip(1.0 - (filler_count * 0.1), 0.0, 1.0))\n",
    "    feedback = f\"Similarity to canonical answer: {similarity_score:.2f}. Expand more on examples and outcomes.\"\n",
    "    suggestions = \"Use STAR structure for behavioral answers; add specific examples and metrics.\"\n",
    "    return {\n",
    "        \"relevance\": relevance,\n",
    "        \"completeness\": completeness,\n",
    "        \"clarity\": clarity,\n",
    "        \"feedback\": feedback,\n",
    "        \"suggestions\": suggestions,\n",
    "        \"matched_canonical_id\": top_canonical.get(\"id\"),\n",
    "        \"similarity\": similarity_score\n",
    "    }\n",
    "\n",
    "def _extract_json_from_text(text: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Try to extract a JSON object from a string (first {...} occurrence).\n",
    "    \"\"\"\n",
    "    match = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    if not match:\n",
    "        return None\n",
    "    candidate = match.group(0)\n",
    "    # Attempt to fix common issues: trailing commas\n",
    "    candidate = re.sub(r\",\\s*}\", \"}\", candidate)\n",
    "    candidate = re.sub(r\",\\s*\\]\", \"]\", candidate)\n",
    "    return candidate\n",
    "\n",
    "def evaluate_answer(question: str, candidate_answer: str, top_k: int = 1) -> Dict:\n",
    "    \"\"\"\n",
    "    Main evaluation function:\n",
    "     - computes embedding similarity to canonical answers\n",
    "     - selects best matching canonical answer(s)\n",
    "     - asks LLM (OpenAI or local) to produce structured evaluation JSON\n",
    "    Returns a dict with:\n",
    "       question, candidate_answer, matched_canonical, similarity, evaluation (scores+feedback)\n",
    "    \"\"\"\n",
    "    candidate_answer = normalize_text(candidate_answer)\n",
    "    ensure_canonical_embeddings()\n",
    "    model = safe_load_embedding_model()\n",
    "    cand_emb = model.encode(candidate_answer, convert_to_numpy=True)\n",
    "\n",
    "    # Compute similarities against canonical embeddings\n",
    "    sims = [cosine_sim(cand_emb, emb) for emb in _canonical_embeddings]\n",
    "    # Top K\n",
    "    idx_sorted = sorted(range(len(sims)), key=lambda i: sims[i], reverse=True)[:top_k]\n",
    "    top_idx = idx_sorted[0]\n",
    "    sim_score = float(sims[top_idx])\n",
    "    top_canonical = CANONICAL_QA[top_idx]\n",
    "\n",
    "    # Call LLM to evaluate using a prompt-based rubric\n",
    "    if OPENAI_KEY and openai:\n",
    "        try:\n",
    "            eval_result = _llm_evaluate_openai(question, candidate_answer, top_canonical, sim_score)\n",
    "        except Exception as e:\n",
    "            logger.warning(\"OpenAI evaluation failed; falling back to local or heuristic. Error: %s\", e)\n",
    "            eval_result = _llm_evaluate_local(question, candidate_answer, top_canonical, sim_score)\n",
    "    else:\n",
    "        eval_result = _llm_evaluate_local(question, candidate_answer, top_canonical, sim_score)\n",
    "\n",
    "    # Normalize numeric fields to floats in [0,1]\n",
    "    for k in (\"relevance\", \"completeness\", \"clarity\", \"similarity\"):\n",
    "        if k in eval_result:\n",
    "            try:\n",
    "                val = float(eval_result[k])\n",
    "                eval_result[k] = max(0.0, min(1.0, val))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"candidate_answer\": candidate_answer,\n",
    "        \"matched_canonical\": {\"id\": top_canonical[\"id\"], \"answer\": top_canonical[\"answer\"]},\n",
    "        \"similarity\": sim_score,\n",
    "        \"evaluation\": eval_result\n",
    "    }\n",
    "\n",
    "# ---------- FastAPI app for integration ----------\n",
    "app = FastAPI(title=\"Step2 LLMs — QuestionGen & Evaluator\")\n",
    "\n",
    "class GenRequest(BaseModel):\n",
    "    round_type: str = \"hr\"\n",
    "    context: Optional[str] = None\n",
    "\n",
    "class EvalRequest(BaseModel):\n",
    "    question: str\n",
    "    candidate_answer: str\n",
    "\n",
    "@app.post(\"/generate_question\")\n",
    "def api_generate_question(req: GenRequest):\n",
    "    return generate_question(req.round_type, req.context)\n",
    "\n",
    "@app.post(\"/evaluate_answer\")\n",
    "def api_evaluate_answer(req: EvalRequest):\n",
    "    return evaluate_answer(req.question, req.candidate_answer)\n",
    "\n",
    "# ---------- CLI demo ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Quick demo usage when run as script\n",
    "    print(\"Step 2 — LLMs demo (generate + evaluate).\")\n",
    "    # 1) Generate a technical question about Python decorators\n",
    "    gen = generate_question(round_type=\"technical\", context=\"Candidate knows Python and web frameworks\")\n",
    "    print(\"Generated question:\", gen[\"question\"], \"(source:\", gen[\"source\"], \")\")\n",
    "\n",
    "    # 2) Example candidate answer (short)\n",
    "    candidate = \"Decorators are functions that wrap another function to extend its behavior without modifying it. They are useful for logging and caching.\"\n",
    "    result = evaluate_answer(gen[\"question\"], candidate)\n",
    "    print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9d47b-f902-4106-8296-3600f2e9c7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cfd246-37e7-4b49-826b-7418b3006590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llms_course_env]",
   "language": "python",
   "name": "conda-env-llms_course_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
