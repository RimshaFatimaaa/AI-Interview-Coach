{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe038493-f8ef-4bcf-be50-0ec6546b910c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rimsha\\anaconda3\\envs\\langchain_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Set your API key here\n",
    "import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"  # replace with your key\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "\n",
    "# ‚úÖ Import functions from your notebook file (make sure LLMs_test.py exists in same dir)\n",
    "from LLMs_test import generate_question, evaluate_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18f0bc6e-83e8-49f2-9118-72512e59f7e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Step 3 ‚Äî LangChain Integration Demo (Latest API Version)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rimsha\\AppData\\Local\\Temp\\ipykernel_13756\\948842048.py:22: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
      "C:\\Users\\rimsha\\AppData\\Local\\Temp\\ipykernel_13756\\948842048.py:45: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:LLMs_test:Loading embedding model 'all-MiniLM-L6-v2' (this may download files)...\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Generated Question:\n",
      "Can you describe a challenging project you worked on and how you overcame the obstacles you faced?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:LLMs_test:Computing embeddings for canonical answers...\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12.46it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 64.76it/s]\n",
      "ERROR:LLMs_test:OpenAI evaluation failed: \n",
      "\n",
      "You tried to access openai.Model, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rimsha\\OneDrive\\Desktop\\AI-Course\\Project-AI-Powered Job Interview Coach\\notebooks\\LLMs_test.py\", line 212, in _llm_evaluate_openai\n",
      "    model=\"gpt-4o\" if \"gpt-4o\" in (openai.Model.list().data[0].id if hasattr(openai,'Model') else \"\") else \"gpt-4o-mini\",\n",
      "                                   ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\rimsha\\anaconda3\\envs\\langchain_env\\Lib\\site-packages\\openai\\lib\\_old_api.py\", line 39, in __call__\n",
      "    raise APIRemovedInV1(symbol=self._symbol)\n",
      "openai.lib._old_api.APIRemovedInV1: \n",
      "\n",
      "You tried to access openai.Model, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Evaluation Result:\n",
      "{\n",
      "  \"question\": \"Can you describe a challenging project you worked on and how you overcame the obstacles you faced?\",\n",
      "  \"candidate_answer\": \"I once resolved a conflict by organizing a team meeting and discussing responsibilities openly.\",\n",
      "  \"matched_canonical\": {\n",
      "    \"id\": \"hr_01\",\n",
      "    \"answer\": \"I used the STAR method: Situation - our team missed a deadline; Task - I coordinated communication; Action - scheduled a meeting, redistributed tasks, negotiated new timelines; Result - we delivered with acceptable quality and learned to align better.\"\n",
      "  },\n",
      "  \"similarity\": 0.4850238550784322,\n",
      "  \"evaluation\": {\n",
      "    \"relevance\": 0.4850238550784322,\n",
      "    \"completeness\": 0.43782298276150583,\n",
      "    \"clarity\": 1.0,\n",
      "    \"feedback\": \"Similarity to canonical answer: 0.49. Expand more on examples and outcomes.\",\n",
      "    \"suggestions\": \"Use STAR structure for behavioral answers; add specific examples and metrics.\",\n",
      "    \"matched_canonical_id\": \"hr_01\",\n",
      "    \"similarity\": 0.4850238550784322\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# üß† 1. Question Generation Chain\n",
    "# -------------------------------------------------------\n",
    "def get_question_generation_chain():\n",
    "    \"\"\"\n",
    "    Creates a chain that generates interview questions using the latest ChatOpenAI.\n",
    "    \"\"\"\n",
    "    template = \"\"\"You are an AI interviewer generating {round_type} interview questions.\n",
    "Candidate context: {context}\n",
    "Generate one clear and concise question. Return only the question text.\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"round_type\", \"context\"],\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    # ‚úÖ Use ChatOpenAI (new API)\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "    # ‚úÖ Use new pipe syntax\n",
    "    chain = prompt | llm\n",
    "    return chain\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# üß© 2. Answer Evaluation Chain\n",
    "# -------------------------------------------------------\n",
    "def get_answer_evaluation_chain():\n",
    "    \"\"\"\n",
    "    Wraps your evaluate_answer() function into a LangChain-like callable.\n",
    "    \"\"\"\n",
    "\n",
    "    def _run(question: str, candidate_answer: str) -> Dict:\n",
    "        # Directly call your notebook function\n",
    "        result = evaluate_answer(question, candidate_answer)\n",
    "        return result\n",
    "\n",
    "    # ‚úÖ Simple chain wrapper\n",
    "    class EvalChain:\n",
    "        def __init__(self):\n",
    "            self.memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
    "\n",
    "        def __call__(self, question: str, candidate_answer: str):\n",
    "            result = _run(question, candidate_answer)\n",
    "            self.memory.chat_memory.add_user_message(candidate_answer)\n",
    "            self.memory.chat_memory.add_ai_message(result[\"evaluation\"][\"feedback\"])\n",
    "            return result\n",
    "\n",
    "    return EvalChain()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# üí¨ 3. Example Usage (Testing)\n",
    "# -------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Step 3 ‚Äî LangChain Integration Demo (Latest API Version)\\n\")\n",
    "\n",
    "    # 1Ô∏è‚É£ Create chains\n",
    "    q_chain = get_question_generation_chain()\n",
    "    eval_chain = get_answer_evaluation_chain()\n",
    "\n",
    "    # 2Ô∏è‚É£ Generate a question\n",
    "    inputs = {\"round_type\": \"HR\", \"context\": \"Candidate is a software engineer with 2 years of experience\"}\n",
    "    question_output = q_chain.invoke(inputs)\n",
    "    generated_question = question_output.content  # get text output from ChatMessage\n",
    "\n",
    "    print(\"\\nüß† Generated Question:\")\n",
    "    print(generated_question)\n",
    "\n",
    "    # 3Ô∏è‚É£ Candidate answer\n",
    "    candidate_ans = \"I once resolved a conflict by organizing a team meeting and discussing responsibilities openly.\"\n",
    "\n",
    "    # 4Ô∏è‚É£ Evaluate answer\n",
    "    evaluation_result = eval_chain(generated_question, candidate_ans)\n",
    "    print(\"\\n‚úÖ Evaluation Result:\")\n",
    "    print(json.dumps(evaluation_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350ad62-debe-44e2-b4b8-b93d44b5b3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:langchain_env]",
   "language": "python",
   "name": "conda-env-langchain_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
